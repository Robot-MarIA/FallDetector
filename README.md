# ğŸš¨ Fall Detection System - YOLO11 Pose

**Sistema de detecciÃ³n de caÃ­das basado en YOLO11 Pose Estimation**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![YOLO11](https://img.shields.io/badge/YOLO-11--Pose-00FFFF.svg)](https://docs.ultralytics.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **TFG** - Sistema diseÃ±ado para detectar caÃ­das y posturas de riesgo en entornos asistenciales, utilizando razonamiento geomÃ©trico sobre keypoints en lugar de clasificaciÃ³n end-to-end.

> [!WARNING]
> **Aviso Legal - Sistema Experimental**
> 
> Este proyecto es un trabajo de investigaciÃ³n acadÃ©mica (TFG).
> 
> **NO es un dispositivo mÃ©dico** y no debe usarse como:
> - Sistema de diagnÃ³stico clÃ­nico
> - Sustituto de supervisiÃ³n humana
> - Sistema de seguridad crÃ­tico sin respaldo
> 
> El sistema puede fallar en detectar caÃ­das o generar falsos positivos.
> Ãšselo bajo su responsabilidad y siempre con supervisiÃ³n humana apropiada.

---

## ğŸš€ Â¡ARRANQUE RÃPIDO!

**â†’ [Leer Tutorial Completo de Arranque](docs/START.md) â†**

```bash
# InstalaciÃ³n rÃ¡pida
py -3 -m venv venv
.\venv\Scripts\Activate.ps1
pip install ultralytics opencv-python pyyaml

# Ejecutar
python main.py --source webcam --show
```

---

## ğŸ“‹ Tabla de Contenidos

- [MotivaciÃ³n](#-motivaciÃ³n)
- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Uso RÃ¡pido](#-uso-rÃ¡pido)
- [Arquitectura](#-arquitectura)
- [CalibraciÃ³n](#-calibraciÃ³n)
- [MigraciÃ³n a ROS2](#-migraciÃ³n-a-ros2)
- [MigraciÃ³n a Jetson](#-migraciÃ³n-a-jetson)
- [Limitaciones](#-limitaciones)

---

## ğŸ¯ MotivaciÃ³n

### Â¿Por quÃ© Pose + Razonamiento?

Los detectores de caÃ­das tradicionales suelen ser:
- Clasificadores binarios entrenados end-to-end
- Cajas negras difÃ­ciles de explicar
- Dependientes de datos de entrenamiento especÃ­ficos

**Este sistema usa un enfoque diferente:**

1. **YOLO-Pose** extrae keypoints (pose estimation pre-entrenada)
2. **Razonamiento geomÃ©trico** analiza la postura (Ã¡ngulos, alturas, proporciones)
3. **ConfirmaciÃ³n temporal** evita falsos positivos

**Ventajas:**
- âœ… **Explicabilidad**: Cada decisiÃ³n tiene una razÃ³n (`TORSO_HORIZONTAL + LOW_HEIGHT`)
- âœ… **GeneralizaciÃ³n**: No depende de dataset especÃ­fico de caÃ­das
- âœ… **Calibrable**: Umbrales ajustables sin reentrenar
- âœ… **Trazabilidad**: Logs detallados para anÃ¡lisis acadÃ©mico

---

## âœ¨ CaracterÃ­sticas

### Estados de Salida
| Estado | Significado | Color |
|--------|-------------|-------|
| `OK` | Sin riesgo detectado | ğŸŸ¢ Verde |
| `RISK` | Posible riesgo, requiere atenciÃ³n | ğŸŸ¡ Naranja |
| `NEEDS_HELP` | Postura de riesgo confirmada | ğŸ”´ Rojo |
| `UNKNOWN` | InformaciÃ³n insuficiente | âšª Gris |

### Posturas Detectadas
- **LYING**: Persona tumbada (horizontal)
- **SITTING_FLOOR**: Sentado en el suelo
- **ALL_FOURS**: A cuatro patas
- **KNEELING**: Arrodillado
- **NORMAL**: De pie, caminando, sentado en silla

### CaracterÃ­sticas TÃ©cnicas
- ğŸ”„ **Temporalidad adaptativa**: Ventana de confirmaciÃ³n dinÃ¡mica (1-5s)
- ğŸ“Š **Quality score**: PenalizaciÃ³n severa sin torso visible
- âš¡ **Scheduler adaptativo**: 3 modos (LOW_POWER, CHECKING, CONFIRMING)
- ğŸ“ **Logs explicables**: CSV/JSON con reason strings
- ğŸ¯ **SelecciÃ³n de persona**: Bbox mÃ¡s grande o mÃ¡s centrado

---

## ğŸš€ InstalaciÃ³n

### Requisitos
- Python 3.8+
- Webcam o archivos de video
- GPU recomendada (tambiÃ©n funciona en CPU)

### Pasos

```bash
# Clonar repositorio
git clone <repo-url>
cd FallDetector

# Crear entorno virtual (recomendado)
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Instalar dependencias
pip install -r requirements.txt

# Descargar modelo YOLO-Pose (automÃ¡tico en primera ejecuciÃ³n)
# El modelo yolo11n-pose.pt se descarga automÃ¡ticamente
```

### Verificar instalaciÃ³n
```bash
# Ejecutar tests
pytest tests/ -v
```

---

## ğŸ® Uso RÃ¡pido

### Con Webcam
```bash
python main.py --source webcam --show
```

### Con Video
```bash
python main.py --source video --path ruta/al/video.mp4 --show
```

### Scripts de Acceso RÃ¡pido (Windows)
```bash
# Webcam
scripts\run_webcam.bat

# Video
scripts\run_video.bat C:\Videos\test.mp4
```

### Opciones Completas
```bash
python main.py --help

# Ejemplos:
python main.py --source webcam --show --verbose
python main.py --source video --path video.mp4 --output logs/
python main.py --source webcam --model yolo11s-pose.pt  # Modelo mÃ¡s preciso
```

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          MAIN.PY                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Frame Source â”‚â”€â”€â”€â–¶â”‚   Pose       â”‚â”€â”€â”€â–¶â”‚   Quality    â”‚      â”‚
â”‚  â”‚  (OpenCV)    â”‚    â”‚  Estimator   â”‚    â”‚  Assessor    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â–¼                   â–¼                   â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Inference    â”‚    â”‚   Feature    â”‚â”€â”€â”€â–¶â”‚  Classifier  â”‚      â”‚
â”‚  â”‚  Backend     â”‚    â”‚  Extractor   â”‚    â”‚              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                             â”‚                   â”‚               â”‚
â”‚                             â–¼                   â–¼               â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                      â”‚   Temporal   â”‚â—€â”€â”€â–¶â”‚  Scheduler   â”‚      â”‚
â”‚                      â”‚   Analyzer   â”‚    â”‚  (Adaptive)  â”‚      â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                             â”‚                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â–¼              â–¼              â–¼                   â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚       â”‚  Output  â”‚   â”‚   Viz    â”‚   â”‚  Logger  â”‚               â”‚
â”‚       â”‚Publisher â”‚   â”‚          â”‚   â”‚ CSV/JSON â”‚               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DiseÃ±o para Escalabilidad

Las **abstracciones** permiten cambiar componentes sin reescribir:

| Componente | PC (Actual) | ROS2 (Futuro) | Jetson (Futuro) |
|------------|-------------|---------------|-----------------|
| Frame Source | `OpenCVFrameSource` | `ROS2ImageSource` | `DeepStreamSource` |
| Inference | `UltralyticsBackend` | `UltralyticsBackend` | `TensorRTBackend` |
| Output | `ConsolePublisher` | `ROS2Publisher` | `ROS2Publisher` |

---

## ğŸšï¸ CalibraciÃ³n

### Archivos de ConfiguraciÃ³n

```
config/
â”œâ”€â”€ thresholds.yaml    # Umbrales de clasificaciÃ³n y calidad
â””â”€â”€ scheduler.yaml     # ConfiguraciÃ³n del scheduler adaptativo
```

### Umbrales Principales (`thresholds.yaml`)

```yaml
pose:
  # Ãngulo del torso (grados desde horizontal)
  torso_angle_lying: 25.0     # < 25Â° = tumbado
  torso_angle_standing: 70.0  # > 70Â° = de pie/normal
  
  # Aspect ratio del bounding box
  aspect_ratio_lying: 1.8     # > 1.8 = orientaciÃ³n horizontal

quality:
  # Requisitos de calidad
  torso_missing_penalty: 0.2  # Sin torso visible â†’ quality Ã— 0.2
  min_quality_for_confirmation: 0.4  # Quality < 0.4 â†’ no confirma NEEDS_HELP
```

### Scheduler (`scheduler.yaml`)

```yaml
modes:
  LOW_POWER:
    fps: 2              # Bajo consumo
    resolution_scale: 0.5
  CHECKING:
    fps: 12             # Verificando
  CONFIRMING:
    fps: 15             # MÃ¡xima atenciÃ³n

transitions:
  # REGLA CLAVE: UNKNOWN + riesgo elevado â†’ CHECKING (nunca LOW_POWER)
  to_checking:
    unknown_with_risk: true
    unknown_risk_threshold: 0.4
```

### CÃ³mo Calibrar

1. **Recoger logs**: Ejecutar con videos de prueba
2. **Analizar CSV**: Revisar `torso_angle`, `risk_score`, `reason`
3. **Ajustar umbrales**: Modificar YAML segÃºn observaciones
4. **No requerir recompilaciÃ³n**: Los cambios aplican al reiniciar

---

## ğŸ¤– MigraciÃ³n a ROS2

El sistema estÃ¡ **preparado** para ROS2 con interfaces abstractas.

### Pasos de MigraciÃ³n

1. **Implementar `ROS2ImageSource`** en `core/frame_source.py`:
```python
class ROS2ImageSource(FrameSource):
    def __init__(self, topic: str = "/camera/image_raw"):
        self.subscription = node.create_subscription(
            Image, topic, self.callback, 10
        )
    
    def get_frame(self) -> Optional[FrameData]:
        # Convertir ROS Image a numpy
        return cv_bridge.imgmsg_to_cv2(self.latest_msg)
```

2. **Implementar `ROS2Publisher`** en `core/outputs.py`:
```python
class ROS2Publisher(OutputPublisher):
    def __init__(self):
        self.state_pub = node.create_publisher(FallState, '/fall_detector/state', 10)
    
    def publish(self, state: SystemState):
        msg = FallState()
        msg.state = state.confirmed_state.value
        msg.risk_score = state.risk_score
        self.state_pub.publish(msg)
```

3. **Crear nodo ROS2** que use el pipeline existente

### Estructura ROS2 Propuesta
```
fall_detector_ros/
â”œâ”€â”€ fall_detector_ros/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ detector_node.py
â”‚   â””â”€â”€ ros2_adapters.py
â”œâ”€â”€ msg/
â”‚   â””â”€â”€ FallState.msg
â”œâ”€â”€ launch/
â”‚   â””â”€â”€ detector.launch.py
â””â”€â”€ package.xml
```

---

## ğŸ”§ MigraciÃ³n a Jetson

Para despliegue en **NVIDIA Jetson** (Nano, Xavier, Orin):

### Paso 1: Exportar a TensorRT

```bash
# En Jetson (o con TensorRT instalado)
yolo export model=yolo11n-pose.pt format=engine device=0
```

### Paso 2: Implementar `TensorRTBackend`

```python
class TensorRTBackend(InferenceBackend):
    def __init__(self, engine_path: str):
        import tensorrt as trt
        self.engine = load_engine(engine_path)
        self.context = self.engine.create_execution_context()
    
    def infer(self, frame: np.ndarray) -> List[PoseDetection]:
        # Preprocessing
        input_tensor = preprocess(frame)
        # TensorRT inference
        outputs = self.context.execute_v2(...)
        # Postprocessing
        return parse_outputs(outputs)
```

### Paso 3: Usar DeepStream (Opcional)

Para mÃ¡ximo rendimiento con mÃºltiples cÃ¡maras:
- Usar DeepStream SDK para pipelines de video
- Hardware-accelerated decoding/encoding
- Mejor eficiencia energÃ©tica

### Consideraciones Jetson

| Aspecto | RecomendaciÃ³n |
|---------|---------------|
| Modelo | `yolo11n-pose` (nano) para tiempo real |
| FP16 | Habilitar para 2x speedup |
| Batch | 1 para mÃ­nima latencia |
| Memoria | Reservar suficiente para TensorRT |

---

## âš ï¸ Limitaciones

### Actuales

1. **Tracking de ID**:
   - Si hay mÃºltiples personas, se selecciona una por frame
   - PodrÃ­a confundir si cambian de posiciÃ³n

2. **Contexto espacial**:
   - No "sabe" dÃ³nde estÃ¡n los muebles
   - Distingue sofÃ¡/cama del suelo por altura (depth), no por semÃ¡ntica

3. **OclusiÃ³n**:
   - Si el torso no es visible, quality baja
   - Puede no detectar postura correctamente

4. **IluminaciÃ³n variable**:
   - Depth puede fallar con luz muy baja
   - En esos casos el sistema pasa a ANALYZING (naranja)

### Mejoras Futuras

- [ ] AÃ±adir tracking con IDs persistentes
- [ ] Mapa semÃ¡ntico del entorno
- [ ] Detector de actividad (caÃ­da vs transiciÃ³n vs acostado)
- [ ] FusiÃ³n con sensores adicionales (audio, PIR)

---

## ğŸ“Š Estructura del Proyecto

```
FallDetector/
â”œâ”€â”€ main.py                 # Punto de entrada principal
â”œâ”€â”€ requirements.txt        # Dependencias Python
â”œâ”€â”€ README.md               # Esta documentaciÃ³n
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ START.md           # ğŸ“– Tutorial de arranque rÃ¡pido
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ thresholds.yaml     # Umbrales de clasificaciÃ³n
â”‚   â””â”€â”€ scheduler.yaml      # ConfiguraciÃ³n del scheduler
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ frame_source.py     # AbstracciÃ³n de fuente de frames
â”‚   â”œâ”€â”€ inference_backend.py # âœ… YOLO11 Pose (lÃ­nea 104, 125)
â”‚   â”œâ”€â”€ pose_estimator.py   # Wrapper YOLO + selecciÃ³n persona
â”‚   â”œâ”€â”€ quality.py          # EvaluaciÃ³n de calidad
â”‚   â”œâ”€â”€ features.py         # ExtracciÃ³n de features
â”‚   â”œâ”€â”€ classifier.py       # ClasificaciÃ³n de poses
â”‚   â”œâ”€â”€ temporal.py         # ConfirmaciÃ³n temporal adaptativa
â”‚   â”œâ”€â”€ scheduler.py        # Scheduler adaptativo
â”‚   â””â”€â”€ outputs.py          # PublicaciÃ³n de resultados
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ geometry.py         # Funciones geomÃ©tricas
â”‚   â”œâ”€â”€ dashboard.py        # ğŸ¨ UI Dashboard (esqueleto coloreado)
â”‚   â”œâ”€â”€ viz.py              # VisualizaciÃ³n bÃ¡sica
â”‚   â””â”€â”€ logging.py          # Logging explicable
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_geometry.py
â”‚   â”œâ”€â”€ test_quality.py
â”‚   â””â”€â”€ test_temporal.py
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ run_webcam.bat
    â””â”€â”€ run_video.bat
```

---

## ğŸ“ Licencia

MIT License - Ver [LICENSE](LICENSE) para detalles.

---

## ğŸ™ Agradecimientos

- [Ultralytics](https://github.com/ultralytics/ultralytics) por YOLO
- COCO Dataset por el formato de keypoints

---

**Desarrollado como Trabajo de Fin de Grado**
